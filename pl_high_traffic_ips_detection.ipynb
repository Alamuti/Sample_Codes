{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Premier League High Traffic IPs Identification\n",
    "# \n",
    "# The present notebook import BigFlow traffic data on the IP level, normalizes the traffic and select \n",
    "# IPs with high volumes of traffic. These IPs are then cross checked with premier league fixtures to \n",
    "# determine if the high traffic is related to premier league football.\n",
    "#\n",
    "# Author: Mohsen Mohammadi - DEC 2020\n",
    "# Version: 2\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "size = 25\n",
    "params = {'legend.fontsize': size,\n",
    "          'figure.figsize': (25,15),\n",
    "          'axes.labelsize': size,\n",
    "          'axes.titlesize': size,\n",
    "          'xtick.labelsize': size*0.75,\n",
    "          'ytick.labelsize': size*0.75,\n",
    "          'axes.titlepad': 25}\n",
    "plt.rcParams.update(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(filename, st, et):\n",
    "    \"\"\"\n",
    "    Read and pre-process input columns into transformed columns.\n",
    "    :param filename: traffic filename\n",
    "    :param st: start time of the interpolation period \n",
    "    :param et: end time of the interpolation period \n",
    "    :return: list of suspicious pirating IPs\n",
    "    \"\"\"    \n",
    "    # Read required traffic datafile to a data-frame\n",
    "    ip_traffic = pd.read_csv(filename, usecols=['bf_date', 'bf_time', 'ip', 'gbps'])\n",
    "    ip_traffic['datetime'] = ip_traffic['bf_date'] + ' ' + ip_traffic['bf_time']\n",
    "    ip_traffic['datetime'] = pd.to_datetime(ip_traffic['datetime'])\n",
    "    ip_traffic['time'] = ip_traffic['datetime'].apply( lambda d : d.time() )\n",
    "    ip_traffic = ip_traffic.drop(columns=['bf_date', 'bf_time'])\n",
    "\n",
    "\n",
    "    #  Pivot and filter IPs based on total or maximum traffic (n x 300GbGb or 20mbs)\n",
    "    ip_pivot = pd.pivot_table(ip_traffic, values='gbps', index=['time'], columns='ip', aggfunc=np.sum)\n",
    "    ip_pivot = ip_pivot[ip_pivot.columns[ip_pivot.max()>0.02]]\n",
    "    ip_pivot = ip_pivot.fillna(0)\n",
    "\n",
    "    \n",
    "    # Normalized IP traffic\n",
    "    s_idx = int(st.hour*12 + st.minute/5 + 1)\n",
    "    e_idx = int(et.hour*12 + et.minute/5 + 1)\n",
    "\n",
    "    ips = ip_pivot[ip_pivot.columns[:]].to_numpy()\n",
    "    ips = ips.T\n",
    "\n",
    "    ips_normalized = preprocessing.minmax_scale(ips.T).T\n",
    "    ips_normalized = pd.DataFrame(ips_normalized.T)\n",
    "    ips_normalized.columns = ip_pivot.columns[:]\n",
    "    ips_normalized.index = ip_pivot.index\n",
    "\n",
    "    # Smoothed IP traffic\n",
    "    ips_smoothed = ips_normalized.groupby(ips_normalized.index).mean().rolling(window=8).mean().shift(periods=-4)\n",
    "\n",
    "    # Differentiating IP traffic to identify sharp rises or declines (over 20 minutes intervals)\n",
    "    ips_d = ips_smoothed.diff(periods=4)\n",
    "\n",
    "    # List of potential pirate IPs (sorted based on number of sharp rises in traffic)\n",
    "    ip_pirate_list = (ips_d[s_idx:e_idx]>.2).sum() \n",
    "    ip_pirate_list = ip_pirate_list[ip_pirate_list>0].sort_values(ascending=False)\n",
    "\n",
    "    return ip_pirate_list.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_date(filename):\n",
    "    \"\"\"\n",
    "    Obtain the match date from the traffic filename.\n",
    "    :param game_date: traffic filename\n",
    "    :return: date of the games\n",
    "    \"\"\"\n",
    "    date = filename[2:4] + '/' + filename[0:2] + '/2020'\n",
    "    return date\n",
    "\n",
    "\n",
    "# ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** **\n",
    "def interpol_periods(date):\n",
    "    \"\"\"\n",
    "    Produce interpolation periods for the given match date using game fixtures. \n",
    "    :param game_date: input date\n",
    "    :return: start time and end time of the interpolation\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('/home/jupyter/preprocessing/data/game_fixtures.csv')\n",
    "    df['match_time_utc'] = pd.to_datetime(df['match_time_utc'])\n",
    "    df = df[df['date']==date].reset_index()\n",
    "    \n",
    "    st = pd.to_datetime(df['ko_time'].iloc[0])\n",
    "    st = st - dt.timedelta(minutes=90)\n",
    "\n",
    "    et = pd.to_datetime(df['ko_time'].iloc[-1])\n",
    "    et = et + dt.timedelta(minutes=180)\n",
    "\n",
    "    return st, et\n",
    "\n",
    "\n",
    "# ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** **\n",
    "def tagger_fn(filename, ip_pirate_list, ip_dbl, ip_bl):\n",
    "    \"\"\"\n",
    "    Create a target column for the traffic file based on normal(0), suspicous(1) and blocked(2) classes. \n",
    "    :param filename: traffic filename\n",
    "    :param ip_pirate_list: list of suspicious pirating IPs\n",
    "    :param ip_dbl: don't black list IPs from FMTS\n",
    "    :param ip_bl: black list IPs from FMTS\n",
    "    :return: traffic file ready for the ML training\n",
    "    \"\"\"\n",
    "    ip_traffic = pd.read_csv(filename, usecols=['bf_date', 'bf_time', 'ip', 'gbps'])\n",
    "    ip_traffic = pd.pivot_table(ip_traffic, values='gbps', index=['ip', 'bf_date'], columns='bf_time', aggfunc=np.sum).fillna(0)\n",
    "\n",
    "    ip_traffic.insert(loc=0, column='target', value=0)\n",
    "    ip_traffic['target'][ip_traffic.index.get_level_values('ip').isin(ip_pirate_list)] = 1\n",
    "    ip_traffic['target'][ip_traffic.index.get_level_values('ip').isin(ip_dbl)] = 1\n",
    "    ip_traffic['target'][ip_traffic.index.get_level_values('ip').isin(ip_bl)] = 2\n",
    "\n",
    "    return ip_traffic\n",
    "    \n",
    "\n",
    "# ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** **\n",
    "def total_traffic_fn(ip_traffic, ip_list):\n",
    "    \"\"\"\n",
    "    Total traffic calculation for a list of IPs.\n",
    "    :param ip_traffic: traffic data per IP\n",
    "    :param ip_list: list of IPs\n",
    "    :return: total traffic (per day) for the provided IPs\n",
    "    \"\"\"\n",
    "    total_traffic = []\n",
    "    for ip in ip_list:\n",
    "        df = ip_traffic[ip_traffic['ip']==ip]\n",
    "        df = df.set_index('time')\n",
    "        df = df.gbps.groupby(df.index.time).sum()\n",
    "        total_traffic.append([ip, df.sum()])\n",
    "                           \n",
    "    return total_traffic\n",
    "\n",
    "\n",
    "# ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** **\n",
    "def downsample(df):\n",
    "    \"\"\"\n",
    "    Down-sample majority class. Also removes the IP traffic from midnight till 8am. \n",
    "    :param df: pivotted traffic data\n",
    "    :param n_samples: the number to be down sampled. \n",
    "    :return: The down sampled database. Also removes the total traffic (per day) for the provided IPs\n",
    "    \"\"\"\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df.target==0]\n",
    "    df_minority1 = df[df.target==1]\n",
    "    df_minority2 = df[df.target==2]\n",
    "\n",
    "    # Downsample majority class\n",
    "    df_majority_downsampled = resample(df_majority, \n",
    "                                     replace=False,         # sample without replacement\n",
    "                                     n_samples=2000000,     # to match minority class\n",
    "                                     random_state=123)      # reproducible results\n",
    " \n",
    "    # Combine minority class with downsampled majority class and remove the IP traffic from midnight till 8am. \n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority1, df_minority2])\n",
    "    df_downsampled = df_downsampled.drop(columns = df_downsampled.columns[0:96])\n",
    "    \n",
    "    return df_downsampled\n",
    "\n",
    "\n",
    "# ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** **\n",
    "def input_eval_fn(filename):\n",
    "    \"\"\"\n",
    "    Prepare the input data to evaluate the Keras model.\n",
    "    :param filename: traffic filename for model evaluation\n",
    "    :return: numpy data for the ML evaluation\n",
    "    \"\"\"\n",
    "    ip_traffic = pd.read_csv(filename, usecols=['time', 'ip', 'gbps'])\n",
    "    ip_traffic['time'] = pd.to_datetime(ip_traffic['time'])\n",
    "    ip_traffic['time'] = ip_traffic['time'].apply( lambda d : d.time() )\n",
    "    ip_traffic = pd.pivot_table(ip_traffic, values='gbps', index=['ip'], columns='time', aggfunc=np.sum).fillna(0)\n",
    "    \n",
    "    # add missing time columns to the ip traffic data\n",
    "    col_list = pd.date_range('00:00', '23:55', freq='5min').time\n",
    "    df_tmp = ip_traffic.reindex(columns=col_list, fill_value=0)\n",
    "    \n",
    "    ip_traffic = pd.merge(df_tmp, ip_traffic, how='outer').fillna(0)\n",
    "    ip_traffic = ip_traffic.drop(columns = ip_traffic.columns[0:96])\n",
    "\n",
    "    # z-norm\n",
    "    x_eval = ip_traffic.values\n",
    "    std_ = x_eval.std(axis=1, keepdims=True)\n",
    "    std_[std_ == 0] = 1.0\n",
    "    x_eval = (x_eval - x_eval.mean(axis=1, keepdims=True)) / std_\n",
    "\n",
    "    if len(x_eval.shape) == 2:  # if univariate\n",
    "    # add a dimension to make it multivariate with one dimension\n",
    "        x_eval = x_eval.reshape((x_eval.shape[0], x_eval.shape[1], 1))\n",
    "    \n",
    "    return x_eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import FMTS (black list/non blak list) IPs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_bl = pd.read_csv('/home/jupyter/preprocessing/data/bl_gw1_9.csv', usecols=['ip'])\n",
    "ip_dbl = pd.read_csv('/home/jupyter/preprocessing/data/dbl_gw1_9.csv', usecols=['ip'])\n",
    "\n",
    "ip_bl = ip_bl.ip.unique()\n",
    "ip_dbl = ip_dbl.ip.unique()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run multiple days of traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pathname = '/home/jupyter/preprocessing/data/'\n",
    "\n",
    "os.chdir(pathname)\n",
    "filenames = [i for i in glob.glob('*.{}'.format('csv'))]\n",
    "filenames = [i for i in filenames if re.match(\"[0-9]+.csv\", i)]\n",
    "\n",
    "count = 0\n",
    "ips = pd.DataFrame()\n",
    "for fname in filenames:\n",
    "    date = match_date(fname)\n",
    "    st, et = interpol_periods(date)    \n",
    "    ip_pirate_list = preprocessing_fn(fname, st, et)\n",
    "    ip_traffic = tagger_fn(fname, ip_pirate_list, ip_dbl, ip_bl)\n",
    "    ips = pd.concat([ips, ip_traffic], ignore_index=True)\n",
    "    count =+ 1\n",
    "    print('traffic datafile no{} \"{}\" processing is completed.'.format(count, fname))\n",
    "\n",
    "\n",
    "ips_train, ips_test = train_test_split(ips, test_size=0.3)\n",
    "ips_train.to_pickle('/home/jupyter/preprocessing/data/ML/IP_TRAIN.pickle')\n",
    "ips_test.to_pickle('/home/jupyter/preprocessing/data/ML/IP_TEST.pickle')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
